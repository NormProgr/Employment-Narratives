{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"bld/python/data/data_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_labels = [\"labor supply\", \"labor demand\", \"government intervention\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/bart-large-mnli\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    return pipe(row, candidate_labels=candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Classification\"] = df[\"Article text\"].apply(process_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Classification\"][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (not useful) handle batches of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16  # Set your desired batch size\n",
    "\n",
    "# Calculate the number of batches needed\n",
    "num_batches = (len(df) + batch_size - 1) // batch_size\n",
    "\n",
    "# Split the DataFrame into batches and process each batch\n",
    "for batch_idx in range(num_batches):\n",
    "    batch_start = batch_idx * batch_size\n",
    "    batch_end = min((batch_idx + 1) * batch_size, len(df))\n",
    "\n",
    "    batch_data = df.iloc[batch_start:batch_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# put to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column = df[\"Article text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = text_column.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_batch = tokenizer(\n",
    "    text_list,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenized_batch[\"input_ids\"]\n",
    "attention_mask = tokenized_batch[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_tensor = input_ids.clone().detach()\n",
    "attention_mask_tensor = attention_mask.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    multi_label=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = classifier(text_list, candidate_labels, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Lecture 7 is key to my problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_from_disk(\"bld/python/data/data_clean\")\n",
    "candidate_labels = [\"labor supply\", \"labor demand\", \"government intervention\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/bart-large-mnli\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"Article text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=model_name,\n",
    "    multi_label=True,\n",
    "    device=\"cuda:0\" if torch.cuda.is_available() else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_classify = (\n",
    "    \"Tiger Woods: Is this the end of his era? - CNN,Tiger Woods is the rarest of athletes. At his peak, he transcended his sport. People who couldn't care less about golf watched in their millions on Sunday afternoons to see him roar. So the 15-time major champ's announcement that he is calling time on life as a full-time pro feels like the end of an era. \",\n",
    "    \"golf, Tiger Woods: Is this the end of his era? - CNN,Is this the end of the Tiger Woods era?,This story was excerpted from the November 23 edition of CNN's Meanwhile in America, the daily email about US politics for global readers. Click here to read past editions and subscribe. (CNN)Tiger Woods is the rarest of athletes. At his peak, he transcended his sport. People who couldn't care less about golf watched in their millions on Sunday afternoons to see him roar. So the 15-time major champ's announcement that he is calling time on life as a full-time pro feels like the end of an era. Woods, who is recuperating from devastating leg injuries from a car crash, told Golf Digest he would have to be more selective about competition from now on. \"\n",
    "    \"I think something that is realistic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(sequence_to_classify, candidate_labels, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functionize it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reasoning for new model\n",
    "\n",
    "https://huggingface.co/valhalla/distilbart-mnli-12-1 has 90% of the facebook/bart-large-mnli model's performance but is way faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_from_disk(\"bld/python/data/data_clean\")\n",
    "model_name = \"facebook/bart-large-mnli\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_2 = \"valhalla/distilbart-mnli-12-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def zero_shot_labelling(data):\n",
    "    model_name = \"valhalla/distilbart-mnli-12-1\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return data.map(\n",
    "        lambda batch: _tokenize(batch, tokenizer),\n",
    "        batched=True,\n",
    "        batch_size=16,  # adjust batch size\n",
    "    )\n",
    "\n",
    "\n",
    "# batch of 8: 47.4, padding = True\n",
    "# batch of 16: 41.3, padding True\n",
    "# batch of 16: 38.4, padding = \"max_length\"\n",
    "\n",
    "\n",
    "def _tokenize(batch, tokenizer):\n",
    "    return tokenizer(batch[\"Description\"], padding=True, truncation=True, max_length=42)\n",
    "\n",
    "\n",
    "# Call zero_shot_labelling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "from transformers import AutoTokenizer, AutoModel, , AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "def zero_shot_labelling(data):\n",
    "    model_name = \"facebook/bart-large-mnli\"\n",
    "    tokenizer = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    return data.map(\n",
    "        lambda batch: _tokenize(batch, tokenizer),\n",
    "        batched=True,\n",
    "        batch_size=16, # adjust batch size\n",
    "    )\n",
    "# batch of 8: 47.4, padding = True\n",
    "# batch of 16: 41.3, padding True\n",
    "# batch of 16: 38.4, padding = \"max_length\"\n",
    "\n",
    "def _tokenize(batch, tokenizer):\n",
    "    return tokenizer(batch[\"Description\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "# Call zero_shot_labelling function\n",
    "df_encoded = zero_shot_labelling(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=model_name_2,\n",
    "    multi_label=True,\n",
    "    device=\"cuda:0\" if torch.cuda.is_available() else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_classify = (\n",
    "    \"Tiger Woods: Is this the end of his era? - CNN,Tiger Woods is the rarest of athletes. At his peak, he transcended his sport. People who couldn't care less about golf watched in their millions on Sunday afternoons to see him roar. So the 15-time major champ's announcement that he is calling time on life as a full-time pro feels like the end of an era. \",\n",
    "    \"golf, Tiger Woods: Is this the end of his era? - CNN,Is this the end of the Tiger Woods era?,This story was excerpted from the November 23 edition of CNN's Meanwhile in America, the daily email about US politics for global readers. Click here to read past editions and subscribe. (CNN)Tiger Woods is the rarest of athletes. At his peak, he transcended his sport. People who couldn't care less about golf watched in their millions on Sunday afternoons to see him roar. So the 15-time major champ's announcement that he is calling time on life as a full-time pro feels like the end of an era. Woods, who is recuperating from devastating leg injuries from a car crash, told Golf Digest he would have to be more selective about competition from now on. \"\n",
    "    \"I think something that is realistic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_labels = [\"labor supply\", \"labor demand\", \"government intervention\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "df_try = df\n",
    "\n",
    "\n",
    "def pd_to_dataset(data):\n",
    "    data = Dataset.from_pandas(data)\n",
    "    dataset_dict = DatasetDict({\"my_dataset\": data})\n",
    "    return dataset_dict[\"my_dataset\"]\n",
    "\n",
    "\n",
    "df_encoded = zero_shot_labelling(df_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(df_encoded[\"Description\"], candidate_labels, tokenizer=_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to speed it up:\n",
    "- Batch size of 8\n",
    "- padding can be reduced to speed up computation\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach to be faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"facebook/bart-large-mnli\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = [{\"Article text\": str(item[\"Article text\"])} for item in df_encoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenizer.encode(df_encoded[\"Article text\"],, return_tensors='pt',\n",
    "                     truncation_strategy='only_first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def zero_shot_labelling(data):\n",
    "    model_name = \"valhalla/distilbart-mnli-12-1\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return data.map(\n",
    "        lambda batch: _tokenize(batch, tokenizer),\n",
    "        batched=True,\n",
    "        batch_size=16,  # adjust batch size\n",
    "    )\n",
    "\n",
    "\n",
    "# batch of 8: 47.4, padding = True\n",
    "# batch of 16: 41.3, padding True\n",
    "# batch of 16: 38.4, padding = \"max_length\"\n",
    "\n",
    "\n",
    "def _tokenize(batch, tokenizer):\n",
    "    return tokenizer(batch[\"Article text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "# Call zero_shot_labelling function\n",
    "df_encoded = zero_shot_labelling(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## free up space every time before running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import scan_cache_dir\n",
    "\n",
    "delete_strategy = scan_cache_dir().delete_revisions(\n",
    "    \"81fd1d6e7847c99f5862c9fb81387956d99ec7aa\"\n",
    "    \"e2983b237dccf3ab4937c97fa717319a9ca1a96d\",\n",
    "    \"6c0e6080953db56375760c0471a8c5f2929baf11\",\n",
    ")\n",
    "print(\"Will free \" + delete_strategy.expected_freed_size_str)\n",
    "\n",
    "\n",
    "delete_strategy.execute()\n",
    "\n",
    "# Specify the directory you want to clear the cache for\n",
    "cache_directory = \"/path/to/your/cache/directory\"\n",
    "\n",
    "# Use scan_cache_dir to clear the cache in the specified directory\n",
    "scan_cache_dir(cache_directory).clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"bld/python/data/data_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aList = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(self.cleaned_data.csv) as f:\n",
    "    reader = csv.reader(f, delimiter=\",\", quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        yield (cell.strip() for cell in row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"cleaned_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.apply(lambda row: len(row[0].split(\",\")) == 4, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internet Notepad Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "\n",
    "df = load_from_disk(\"bld/python/data/data_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv(\"bld/python/data/data_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")  # choose model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = df_csv[[\"Article text\", \"Category\"]]\n",
    "df_csv[\"Category\"] = df_csv[\"Category\"].astype(\"category\")\n",
    "df_csv[\"text\"] = df_csv[\"Article text\"]\n",
    "df_csv = df_csv.drop(\"Article text\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv[\"Category_code\"] = df_csv.Category\n",
    "df_csv[\"Category_code\"] = df_csv.Category.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv[\"label\"] = df_csv[\"Category_code\"]\n",
    "df_csv = df_csv.drop(\"Category_code\", axis=1)\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "\n",
    "\n",
    "def tokenize_data_2(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = df_csv.map(tokenize_data_2, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuer try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "df = load_from_disk(\"bld/python/data/data_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = zero_shot_labelling(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def zero_shot_labelling(data):\n",
    "    model_name = \"valhalla/distilbart-mnli-12-1\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return data.map(\n",
    "        lambda batch: _tokenize(batch, tokenizer),\n",
    "        batched=True,\n",
    "        batch_size=16,  # adjust batch size\n",
    "    )\n",
    "\n",
    "\n",
    "# batch of 8: 47.4, padding = True\n",
    "# batch of 16: 41.3, padding True\n",
    "# batch of 16: 38.4, padding = \"max_length\"\n",
    "\n",
    "\n",
    "def _tokenize(batch, tokenizer):\n",
    "    return tokenizer(batch[\"Description\"], padding=True, truncation=True, max_length=42)\n",
    "\n",
    "\n",
    "# Call zero_shot_labelling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_labels = [\"labor supply\", \"labor demand\", \"government intervention\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_2 = \"valhalla/distilbart-mnli-12-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=model_name_2,\n",
    "    multi_label=True,\n",
    "    device=\"cuda:0\" if torch.cuda.is_available() else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(df[\"Description\"], candidate_labels, tokenizer=_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## just functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "df = load_from_disk(\"bld/python/data/data_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_entries(dataframe, num_entries=50, random_state=42):\n",
    "    \"\"\"Select a random set of entries from a Pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe (pd.DataFrame): The input DataFrame with 6 columns.\n",
    "    - num_entries (int): The number of random entries to select (default is 50).\n",
    "    - random_state (int or None): Random seed for reproducibility (default is None).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing the randomly selected entries.\n",
    "    \"\"\"\n",
    "    dataframe = pd.DataFrame(dataframe)\n",
    "    # dataframe is json\n",
    "\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)  # Set the random seed\n",
    "\n",
    "    # Check if num_entries is greater than the total number of rows\n",
    "    if num_entries > len(dataframe):\n",
    "        msg = \"Number of entries to select cannot exceed the total number of rows.\"\n",
    "        raise ValueError(\n",
    "            msg,\n",
    "        )\n",
    "\n",
    "    # Use Pandas' sample method to select random entries\n",
    "    return dataframe.sample(n=num_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_random_entries(df, num_entries=50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_100_entries = df.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class= zero_shot_classifier(first_100_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def zero_shot_classifier(data):\n",
    "    \"\"\"Classify the zero-shot data to receive the labels.\"\"\"\n",
    "    data = _zero_shot_labelling(data)\n",
    "    model_name = \"valhalla/distilbart-mnli-12-6\"\n",
    "    labels = [\"labor supply\", \"labor demand\", \"government intervention\"]\n",
    "    classifier = pipeline(  # second last\n",
    "        \"zero-shot-classification\",\n",
    "        model=model_name,\n",
    "        multi_label=True,\n",
    "        device=\"cuda:0\" if torch.cuda.is_available() else None,\n",
    "    )\n",
    "    return classifier(  # last\n",
    "        data[\"Description\"],\n",
    "        labels,\n",
    "        tokenizer=_tokenize,\n",
    "    )\n",
    "\n",
    "\n",
    "def _zero_shot_labelling(data):\n",
    "    \"\"\"Load the model for zero-shot classification and apply on the data.\"\"\"\n",
    "    model_name = \"valhalla/distilbart-mnli-12-6\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return data.map(\n",
    "        lambda batch: _tokenize(batch, tokenizer),\n",
    "        batched=True,\n",
    "        batch_size=8,\n",
    "    )\n",
    "\n",
    "\n",
    "def _tokenize(batch, tokenizer):\n",
    "    \"\"\"Define the tokenizer.\"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"Description\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For reading to know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the previous\n",
    "select_random_entries(df, num_entries=50, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- does the probability of the model suits or should I transform to 0 and 1\n",
    "- test and training separation\n",
    "- model selection\n",
    "- put the head on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"bld\\\\python\\\\labelled\\\\data_labelled_subset.json\"\n",
    "\n",
    "df = pd.read_json(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "# better way with arrays\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"sequence\"],\n",
    "    df[\"scores\"],\n",
    "    test_size=0.15,\n",
    "    random_state=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adadd = load_from_disk(\"bld/python/labelled/data_labelled_subset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "data = load_from_disk(\"bld/python/data/data_clean\")\n",
    "first_100_entries = data.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = zero_shot_classifier(first_100_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_labelling(first_100_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_labelling(data):\n",
    "    \"\"\"Load the model for zero-shot classification and apply on the data.\"\"\"\n",
    "    model_name = \"valhalla/distilbart-mnli-12-6\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    df_encoded = data.map(\n",
    "        lambda batch: _tokenize(batch, tokenizer),\n",
    "        batched=True,\n",
    "        batch_size=8,\n",
    "    )\n",
    "    df_encoded.set_format(\n",
    "        \"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\"],\n",
    "    )\n",
    "    df_encoded.set_format(\"torch\")\n",
    "    return df_encoded\n",
    "\n",
    "\n",
    "def _tokenize(batch, tokenizer):\n",
    "    \"\"\"Define the tokenizer.\"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"Description\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Functions for fitting the regression model.\"\"\"\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "def _zero_shot_labelling(data):\n",
    "    \"\"\"Load the model for zero-shot classification and apply on the data.\"\"\"\n",
    "    model_name = \"valhalla/distilbart-mnli-12-6\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    df_encoded = data.map(\n",
    "        lambda batch: _tokenize(batch, tokenizer),\n",
    "        batched=True,\n",
    "        batch_size=8,\n",
    "    )\n",
    "    df_encoded.set_format(\n",
    "        \"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\"],\n",
    "    )\n",
    "    df_encoded.set_format(\"torch\")\n",
    "    return df_encoded\n",
    "\n",
    "\n",
    "def _tokenize(batch, tokenizer):\n",
    "    \"\"\"Define the tokenizer.\"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"Description\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"bld\\\\python\\\\labelled\\\\data_labelled_subset.json\"\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(file_path) as json_file:\n",
    "    df = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"sequence\"],\n",
    "    df[\"scores\"],\n",
    "    test_size=0.15,\n",
    "    random_state=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "\n",
    "\n",
    "def tokenize_data(df):\n",
    "    return tokenizer(\n",
    "        df[\"sequence\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.map(tokenize_data, batched=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
